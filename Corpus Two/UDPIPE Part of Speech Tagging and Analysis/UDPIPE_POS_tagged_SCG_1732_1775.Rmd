---
title: "UDPIPE_POS_tagged_SCG_1732_1775"
author: "Chloe Zehr"
date: "2024-09-21"
output: html_document
---

necessary libraries:
```{r}

library(udpipe) #part of speech tagging
library(tidyverse) #general utility and tokenization
library(dplyr) #general utility
library(tm) #text mining
library(stringr) #manipulating strings
library(ggplot2) #basic visualizations
library(NLP) #natural language processing tools
library(ggraph) # for graphing cooccurrence
library(igraph) # for graphing cooccurrence
library(readxl) #for reading excel sheets
library(extrafont) #for fonts
library(gganimate) #animating data
library(widyr) #for pairwise
library(ggrepel) #for graphing aesthetics with word
library(quanteda) #works with text data and has a built in concordance feature
library(textstem) #lemmatizing for concordance

```


loading data:
```{r}
#file.choose()

#general dataset
CorpusOne <- read_excel("C:/Users/chloe/OneDrive - UCB-O365/Masters Thesis 2024/Computational Text Mining/Corpora_final/Corpus_One.xlsx", sheet = "sheet1") 

```

General EDA: 
```{r}
#reference type categories in reports that make mention of enslaved resistance
refence_type_count <- CorpusOne %>% 
  select(reference_type) %>% 
  group_by(reference_type) %>%
  count(reference_type) %>%
  arrange(desc(n))

```

Concordances: 
```{r}

CorpusOne$text_lemma <- lemmatize_strings(CorpusOne$text) #trying to lemmatize
corpus_lemma <- corpus(CorpusOne, text_field = "text_lemma")
tokens_lemma <- quanteda::tokens(corpus_lemma, remove_punct = TRUE)
concordance_dfm <- dfm(tokens_lemma) #creates a document frame matrix

#exploring target words:
#insurrection
insurrections_con <- kwic(concordance_tokens, pattern = "insurrections", window = 6) 
print(insurrections_con)
 
#we
we_con <- kwic(concordance_tokens, pattern = "we", window = 6) 
print(we_con)

#our
our_con <- kwic(concordance_tokens, pattern = "our", window = 6) 
print(our_con)

#suppress
suppress_con <- kwic(concordance_tokens, pattern = "suppress", window = 6) 
print(suppress_con)

#subdue
subdue_con <- kwic(tokens_lemma, pattern = "subdue", window = 6) 

#prevent
prevent_con <- kwic(tokens_lemma, pattern = "prevent", window = 6) 

#black
black_con <- kwic(tokens_lemma, pattern = "black", window = 6) 

#african
african_con <- kwic(tokens_lemma, pattern = "africa", window = 6) 


```


Word co-occurence over time (POS tagging with UDPIPE)
1. loading data and model:
```{r}
#selecting data
text_time <- CorpusOne %>% 
  select(text, year)

#load UDPIPE language model if needed: 
model <- udpipe_download_model(language = "english", overwrite = FALSE)
ud_model <- udpipe_load_model(model$file_model) 
```

2. Annotating the corpus while maintaining year information:
```{r}
# Annotate with UDPIPE for text data and add year information
pos_annotate_year <- function(text, year, model) { #new function for adding the year variable
  annotated <- as.data.frame(udpipe_annotate(model, x = text))
  annotated$year <- year  # Add the year column
  return(annotated)
}

# Apply annotation to each text and maintain year information; iterates over each text
text_time$annotation_df <- mapply(pos_annotate_year, text_time$text, text_time$year, MoreArgs = list(model = ud_model), SIMPLIFY = FALSE)
```

3. Removing any invalid annotations
```{r}

# Check for invalid annotations
invalid_annotations <- sapply(text_time$annotation_df, function(x) is.null(x) || nrow(x) == 0)

# Filter the data frame based on valid annotations
valid_indices <- which(!invalid_annotations)
text_time <- text_time[valid_indices, ]

# Print out the texts that caused problems
if (any(invalid_annotations)) {
  print("Texts with invalid annotations:")
  print(text_time$text[invalid_annotations])
}

```

4. Combining all the annotated data for each row (or text) into one df that maintains year information
```{r}

# Use bind_rows to combine the list of dataframes - this can bypass issues with memory management and relatively large dataframes like those of UDPIPE
annotated_data <- bind_rows(text_time$annotation_df)

# Inspect the first few rows
head(annotated_data)

#making all lemmas lowercase
annotated_data$lemma <- tolower(annotated_data$lemma)

```


5. Visualize in 10-year intervals: (NOTE: there are no articles in the SCG published that refer to collective enslaved resistance in 1774 and 1775 - in addition, there are years and decades with very few articles, sometimes zero or only 1, making noun and adjective co-occurence not a very useful measurement). 

1732-1742
```{r}
annotated_1732_1742 <- annotated_data %>% filter(year >= 1732 & year <= 1742)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1732_1742 <- cooccurrence(annotated_1732_1742$lemma, relevant = annotated_1732_1742$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1732_1742, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_NOUN_ADJ_cooc_1732_1734.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1732_1742 <- cooccurrence(annotated_1732_1742$token, relevant = annotated_1732_1742$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1732_1742, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_PRON_VERB_cooc_1732_1742.svg", plot = last_plot(), width = 6, height = 4)

```

1743-1753
```{r}
annotated_1743_1753 <- annotated_data %>% filter(year >= 1743 & year <= 1753)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1743_1753 <- cooccurrence(annotated_1743_1753$lemma, relevant = annotated_1743_1753$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1743_1753, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_noun_adj_cooc_1743_1753.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1743_1753 <- cooccurrence(annotated_1743_1753$token, relevant = annotated_1743_1753$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1743_1753, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_pron_verb_cooc_1743_1753.svg", plot = last_plot(), width = 6, height = 4)

```

1754-1764
```{r}
annotated_1754_1764 <- annotated_data %>% filter(year >= 1754 & year <= 1764)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1754_1764 <- cooccurrence(annotated_1754_1764$lemma, relevant = annotated_1754_1764$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1754_1764, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_noun_adj_cooc_1754_1764.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1754_1764 <- cooccurrence(annotated_1754_1764$token, relevant = annotated_1754_1764$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1754_1764, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_pron_verb_cooc_1754_1764.svg", plot = last_plot(), width = 6, height = 4)

```

1765-1775
```{r}

annotated_1765_1775 <- annotated_data %>% filter(year >= 1765 & year <= 1775)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1765_1775 <- cooccurrence(annotated_1765_1775$lemma, relevant = annotated_1765_1775$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1765_1775, 35) #because all bigrams only occur once I expanded to include insurrection reference; not a very useful visualization
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_noun_adj_cooc_1765_1775.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1765_1775 <- cooccurrence(annotated_1765_1775$token, relevant = annotated_1765_1775$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1765_1775, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusOne_pron_verb_cooc_1765_1775.svg", plot = last_plot(), width = 6, height = 4)

```

The use of pronouns:
```{r}
#WE
# Filter for "we" lemma and make all tokens lowercase
annotated_we <- annotated_data %>%
  filter(str_to_lower(lemma) == "we") %>%   # Convert lemma to lowercase during filtering
  mutate(token = str_to_lower(token),       # Convert token to lowercase
         lemma = str_to_lower(lemma))       # Ensure lemma is lowercase as well

# Splitting into 10-year intervals
annotated_we <- annotated_we %>%
  mutate(interval = cut(year, 
                        breaks = c(1732, 1742, 1752, 1762, 1772, 1775),  # Define custom breaks for 10-year intervals
                        include.lowest = TRUE, 
                        labels = c("1732-1741", "1742-1751", "1752-1761", "1762-1771", "1772-1775")))

#graph over time
ggplot(annotated_we) + 
  geom_bar(mapping = aes(token)) + 
  facet_wrap(~interval, scales = "free_x") +   # Free x-scales to display x-axis on every facet rather than just at the bottom 
  labs(x = "Token", y = "Count") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11, color = "black"),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),         # Axis text font size
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11),      # Legend title font size
    panel.spacing = unit(1.5, "lines"),          # Adjust space between facets (increase as needed)
    strip.text = element_text(size = 12),        # Facet labels font size
    axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis text if needed
  )

ggsave("CorpusOne_We_intervals.svg", plot = last_plot(), width = 6, height = 4)


#general pronouns (pronouns, indefinite pronouns, demonstrative pronouns)
annotated_pron <- annotated_data %>% filter(upos == "PRON")
annotated_pron <- annotated_pron %>% mutate(token = str_to_lower(token))
annotated_pron_count <- annotated_pron %>% 
  group_by(token) %>%
  count(token) %>%
  arrange(desc(n))

pron_top30 <- head(annotated_pron_count, 30)

#most frequent pronouns in the corpus: 
ggplot(pron_top30) + 
  geom_col(mapping = aes(x = n, y = fct_reorder(token, n))) + 
  labs(x = "Count", y = "Pronoun") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )

ggsave("CorpusOne_gen_pronouns.svg", plot = last_plot(), width = 6, height = 4)


```

The types of articles that contain "we": 
```{r}

we_filtered <- CorpusOne %>% 
  filter(str_detect(str_to_lower(text), "\\b(we|us|our)\\b")) #makes lowercase and uses regex to find rows that contain forms of "we" 

we_count <- we_filtered %>% 
  group_by(reference_type) %>%
  count(reference_type) %>% 
  arrange(desc(n))

```


Examining sentences that contain target words: Syntactic Dependency Relationships
```{r}

#creating set of target lemmas related to enslaved resistance
target_lemmas <- c("insurrection", "insurrections", "plott", "slave", "slaves", "negro", "negroes", "rebellious", "intended", "massacre", "conspiracy", "conspiracies", "rebel", "attempts", "rising", "plot", "uprising", "wicked", "ringleader", "king", "leader", "whites", "discovered", "suppress")

#filtering dataset for observations that contain target words
filtered_dat <- annotated_data %>%
  filter(lemma %in% target_lemmas)

#frequency of dependency relationships
dep_rel_count <- filtered_dat %>%
  count(lemma, dep_rel) %>%
  arrange(desc(n))

#dependency relationships by year (not the most useful information since it decontextualizes sentences)
dep_rel_year <- filtered_dat %>%
  group_by(year, lemma, dep_rel) %>%
  summarise(count = n(), .groups = "drop") %>% 
  arrange(year, desc(count))

#looking at relationships between two words in a sentence
si_target_lemmas <- c("slave", "insurrection")

#finding sentence_ID's with both words present
sentences_si <- annotated_data %>%
  group_by(sentence_id) %>%
  filter(any(lemma == "slave") & any(lemma == "insurrection"))

#getting dependency relationships for "slave" and "insurrection" when they are used in a sentence together
dep_rel_si <- sentences_si %>%
  filter(lemma %in% si_target_lemmas) %>%
  select(sentence_id, year, lemma, dep_rel)

#analyzing their relationships overtime
time_dep_rel_si <- dep_rel_si %>%
  group_by(year, sentence_id, lemma) %>%
  summarise(dep_rel_count = n(), .groups = 'drop') %>%
  arrange(year, desc(dep_rel_count))

#visualizing trends in the dep_rel between the lemmas "slave" and "insurrection"
ggplot(dep_rel_si) + 
  geom_bar(mapping = aes(x = dep_rel, fill = lemma))


###### how dependency relations changed over time between the lemmas insurrection, rebel, rebellious, negro, slave
target_lemmas2 <- c("insurrection", "slave", "rebel", "rebellious", "negro", "negroes") #note that slave is the lemma for "slaves" 

filtered_dat2 <- annotated_data %>%
  filter(lemma %in% target_lemmas2)

dep_trends <- filtered_dat2 %>%
  group_by(year, lemma, dep_rel) %>%
  summarise(count = n(), .groups = "drop")

# Plot dependency relationships over time
ggplot(dep_trends, aes(x = year, y = count, fill = dep_rel)) +
  geom_col() +
  facet_wrap(~ lemma, scales = "fixed") +
  theme_minimal() + 
  labs(x = "Year", y = "Count") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), #adding 45 degree angle to x-axis
    axis.ticks.x = element_line(size = 0.5), #x-axis ticks
    panel.spacing = unit(1, "lines"), #adding more space between each facet
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )


ggsave("CorpusOne_dep_rel_Counts.svg", plot = last_plot(), width = 6, height = 4)


```




Workshopping: animation over time: 
```{r}
# Step 1: Isolate nouns and adjectives, keeping 'doc_id', 'sentence_id', and 'year'
noun_adj <- annotated_data %>%
  filter(upos %in% c("NOUN", "ADJ")) %>%
  select(doc_id, sentence_id, token, lemma, upos, year)

# Step 2: Group the data by 'year' and 'sentence_id' BEFORE calling pairwise_count
# We need to ensure that each co-occurrence is calculated within its specific 'year' and 'sentence_id' context
noun_adj_grouped <- noun_adj %>%
  group_by(year, sentence_id)

# Step 3: Create co-occurrence of bigrams (pairwise lemma count)
noun_adj_cooc <- noun_adj_grouped %>%
  pairwise_count(lemma, sentence_id, sort = TRUE)

# Step 4: Ensure 'year' is part of the resulting dataframe
# Since we've grouped by 'year' before, it should now be retained
# Inspect the first few rows
head(noun_adj_cooc)

# Converting to graph form
noun_adj_graph <- graph_from_data_frame(noun_adj_cooc, directed = FALSE)

# Add 'year' as an edge attribute (it's already part of navp_cooc)
E(noun_adj_graph)$year <- noun_adj_cooc$year

# Checking if 'year' is retained in the new object 
head(E(noun_adj_graph)$year) #this is part of potential debugging

# Create the network plot
noun_adj_network <- ggraph(noun_adj_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
  geom_node_point(color = "darkblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, size = 5) +
  theme_void()

# Add animation over time using the 'year' edge attribute
noun_adj_animated <- noun_adj_network + 
  transition_time(as.numeric(E(noun_adj_graph)$year)) +  # Use the 'year' edge attribute
  labs(title = 'Co-occurrence Network: Year 1732-1775')

# Animate and save as a gif
animate(noun_adj_animated, width = 800, height = 600, fps = 5)


```


Word Co-Occurence Network for Nouns and Adjectives across the whole corpus:
```{r}

# Filter for POS tags and nouns/adjectives that follow one another
Cooccurrence <- cooccurrence(CorpusOne_annotations_df$lemma, relevant = CorpusOne_annotations_df$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm


# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(Cooccurrence, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )


ggsave("CorpusOne_NOUN_ADJ_coocurrence.svg", plot = last_plot(), width = 6, height = 4)


```