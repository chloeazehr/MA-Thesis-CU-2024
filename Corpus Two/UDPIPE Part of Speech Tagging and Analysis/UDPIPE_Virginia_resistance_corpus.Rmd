---
title: "UDPIPE_Virginia_Resistance_Corpus"
author: "Chloe Zehr"
date: "2024-09-11"
output: html_document
---

Loading necessary libraries:
```{r}

library(udpipe) #part of speech tagging
library(tidyverse) #general utility and tokenization
library(dplyr) #general utility
library(tm) #text mining
library(stringr) #manipulating strings
library(ggplot2) #basic visualizations
library(NLP) #natural language processing tools
library(ggraph) # for graphing cooccurrence
library(igraph) # for graphing cooccurrence
library(readxl) #for reading excel sheets
library(extrafont) #for fonts
library(gganimate) #animating data - this is something I will try more in the future
library(widyr) #for pairwise
library(ggrepel) #for graphing aesthetics with word
library(quanteda) #works with text data and has a built in concordance feature
library(textstem) #lemmatizing for concordance

```

loading data:
```{r}
file.choose()

CorpusTwo <- read.csv("C:\\Users\\chloe\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\Corpora_final\\Corpus_Two_csv.csv")

```

EDA & Concordance: 
```{r}

#reference type categories in reports that make mention of enslaved resistance
refence_type_count <- CorpusTwo %>% 
  select(reference_type) %>% 
  group_by(reference_type) %>%
  count(reference_type) %>%
  arrange(desc(n))

#concordance analyses: 

CorpusTwo$text <- iconv(CorpusTwo$text, from = "latin1", to = "UTF-8", sub = "") #needs UTF-8 encoding
CorpusTwo$text_lemma <- lemmatize_strings(CorpusTwo$text)
corpus_lemma <- corpus(CorpusTwo, text_field = "text_lemma")
tokens_lemma <- quanteda::tokens(corpus_lemma, remove_punct = TRUE)
concordance_dfm <- dfm(tokens_lemma)

#exploring target words:
#insurrection
insurrections_con <- kwic(tokens_lemma, pattern = "insurrection", window = 6) 
print(insurrections_con)

#we
we_con <- kwic(tokens_lemma, pattern = "we", window = 6) 
print(we_con)

#our
our_con <- kwic(tokens_lemma, pattern = "our", window = 6) 
print(our_con)

#suppress
suppress_con <- kwic(tokens_lemma, pattern = "suppress", window = 6) 
print(suppress_con)

#subdue
subdue_con <- kwic(tokens_lemma, pattern = "subdue", window = 6) 

#prevent
prevent_con <- kwic(tokens_lemma, pattern = "prevent", window = 6)

#black
black_con <- kwic(tokens_lemma, pattern = "black", window = 6) 

#african
african_con <- kwic(tokens_lemma, pattern = "africa", window = 6) 

```

Word co-occurence over time (POS tagging with UDPIPE)
1. loading data and model:
```{r}
#selecting data
text_time2 <- CorpusTwo %>% 
  select(text, year)

#load UDPIPE language model if needed: 
model <- udpipe_download_model(language = "english", overwrite = FALSE)
ud_model <- udpipe_load_model(model$file_model) 
```

2. Annotating the corpus while maintaining year information: (had to add assurance for UTF-8 encoding)
```{r}
# Function to annotate text and add year information
pos_annotate_year <- function(text, year, model) {
  text_utf8 <- iconv(text, from = "latin1", to = "UTF-8", sub = "")
  annotated <- as.data.frame(udpipe_annotate(model, x = text_utf8))
  annotated$year <- year
  return(annotated)
}

# Apply annotation to each text and maintain year information
text_time2$annotation_df <- mapply(pos_annotate_year, text_time2$text, text_time2$year, 
                                   MoreArgs = list(model = ud_model), SIMPLIFY = FALSE)

```

3. Removing any invalid annotations - to avoid errors
```{r}

# Check for invalid annotations
invalid_annotations <- sapply(text_time2$annotation_df, function(x) is.null(x) || nrow(x) == 0)

# Filter the data frame based on valid annotations
valid_indices <- which(!invalid_annotations)
text_time2 <- text_time[valid_indices, ]

# Print out the texts that caused problems
if (any(invalid_annotations)) {
  print("Texts with invalid annotations:")
  print(text_time$text[invalid_annotations])
}

```

4. Combining all the annotated data for each row (or text) into one df that maintains year information
```{r}
# Use bind_rows to combine the list of dataframes - this can bypass issues with memory management and relatively large dataframes like those of UDPIPE
CorpusTwo_annotated_data <- bind_rows(text_time2$annotation_df)

# Inspect the first few rows
head(CorpusTwo_annotated_data)

#making all lemmas lowercase
CorpusTwo_annotated_data$lemma <- tolower(CorpusTwo_annotated_data$lemma) #lemmas lowercase
CorpusTwo_annotated_data$token <- tolower(CorpusTwo_annotated_data$token) #tokens lowercase
```


5. Visualize in 10-year intervals: (NOTE: there are no articles in the Virginia Gazette(s) that reference collective enslaved resistance until 1737 even though William Parks began publishing in 1735. In addition, there are years and decades with very few articles, sometimes zero or only 1, making noun and adjective co-occurence not a very useful measurement since many terms were hapaxes in the article). 

1735-1745
```{r}
annotated_1735_1745 <- CorpusTwo_annotated_data %>% filter(year >= 1735 & year <= 1745)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1735_1745 <- cooccurrence(annotated_1735_1745$lemma, relevant = annotated_1735_1745$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1735_1745, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_NOUN_ADJ_cooc_1735_1745.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1735_1745 <- cooccurrence(annotated_1735_1745$token, relevant = annotated_1735_1745$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1735_1745, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_PRON_VERB_cooc_1735_1745.svg", plot = last_plot(), width = 6, height = 4)

```

1746-1756
```{r}
annotated_1746_1756 <- CorpusTwo_annotated_data %>% filter(year >= 1746 & year <= 1756)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1746_1756 <- cooccurrence(annotated_1746_1756$lemma, relevant = annotated_1746_1756$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1746_1756, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_noun_adj_cooc_1746_1756.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1746_1756 <- cooccurrence(annotated_1746_1756$token, relevant = annotated_1746_1756$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1746_1756, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_pron_verb_cooc_1746_1756.svg", plot = last_plot(), width = 6, height = 4)

```

1757-1767
```{r}
annotated_1757_1767 <- CorpusTwo_annotated_data %>% filter(year >= 1757 & year <= 1767)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1757_1767 <- cooccurrence(annotated_1757_1767$lemma, relevant = annotated_1757_1767$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1757_1767, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_noun_adj_cooc_1757_1767.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1757_1767 <- cooccurrence(annotated_1757_1767$token, relevant = annotated_1757_1767$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1757_1767, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_pron_verb_cooc_1757_1767.svg", plot = last_plot(), width = 6, height = 4)

```

1768-1775
```{r}

annotated_1768_1775 <- CorpusTwo_annotated_data %>% filter(year >= 1768 & year <= 1775)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1768_1775 <- cooccurrence(annotated_1768_1775$lemma, relevant = annotated_1768_1775$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1768_1775, 30) #because all bigrams only occur once I expanded to include insurrection reference; not a very useful visualization
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_noun_adj_cooc_1768_1775.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1768_1775 <- cooccurrence(annotated_1768_1775$token, relevant = annotated_1768_1775$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1768_1775, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_pron_verb_cooc_1768_1775.svg", plot = last_plot(), width = 6, height = 4)

```

The use of pronouns:
```{r}
#general pronouns (pronouns, indefinite pronouns, demonstrative pronouns)
annotated_pron <- CorpusTwo_annotated_data %>% filter(upos == "PRON")
annotated_pron <- annotated_pron %>% mutate(token = str_to_lower(token))
annotated_pron_count <- annotated_pron %>% 
  group_by(token) %>%
  count(token) %>%
  arrange(desc(n))

pron_top30 <- head(annotated_pron_count, 30)

#most frequent pronouns in the corpus: 
ggplot(pron_top30) + 
  geom_col(mapping = aes(x = n, y = fct_reorder(token, n))) + 
  labs(x = "Count", y = "Pronoun") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )

ggsave("CorpusTwo_gen_pronouns.svg", plot = last_plot(), width = 6, height = 4)


```

The types of articles that contain "we": 
```{r}

#primarily editorials

we_filtered <- CorpusTwo %>% 
  filter(str_detect(str_to_lower(text), "\\b(we|us|our)\\b")) #makes lowercase and uses regex to find rows that contain forms of "we" 

we_count <- we_filtered %>% 
  group_by(reference_type) %>%
  count(reference_type) %>% 
  arrange(desc(n))

```


Examining sentences that contain target words: Syntactic Dependency Relationships
```{r}
# dependency relations changed over time between the lemmas insurrection, rebel, rebellious, negro, slave
CorpusTwo_target_lemmas <- c("insurrection", "slave", "rebel", "rebellious", "negro", "negroes") #note that slave is the lemma for "slaves" 

CorpusTwo_filtered_dat <- CorpusTwo_annotated_data %>%
  filter(lemma %in% CorpusTwo_target_lemmas)

CorpusTwo_dep_trends <- CorpusTwo_filtered_dat %>%
  group_by(year, lemma, dep_rel) %>%
  summarise(count = n(), .groups = "drop")

# Plot dependency relationships over time
ggplot(CorpusTwo_dep_trends, aes(x = year, y = count, fill = dep_rel)) +
  geom_col() +
  facet_wrap(~ lemma, scales = "fixed") +
  theme_minimal() + 
  labs(x = "Year", y = "Count") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), #adding 45 degree angle to x-axis
    axis.ticks.x = element_line(size = 0.5), #x-axis ticks
    panel.spacing = unit(1, "lines"), #adding more space between each facet
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )


ggsave("CorpusTwo_dep_rel_Counts.svg", plot = last_plot(), width = 6, height = 4)


```



























Workshopping:
```{r}
# Add sentence boundaries
v_annotations_df <- v_annotations_df %>%
  mutate(sentence_id = cumsum(upos == "Punct" & !is.na(upos)))

v_target_words <- c("insurrection", "insurrections", "plott", "slave", "slaves", "negro", "negroes", "rebellious", "intended", "massacre", "conspiracy", "rebel", "plot", "uprising", "wicked")

# Filter sentences containing the word "insurrection"
sentences_with_insurrection <- v_annotations_df %>%
  filter(str_detect(lemma, str_c(v_target_words, collapse = "|"))) %>%
  select(sentence_id) %>%
  distinct()

# Filter original annotations to include only sentences with "insurrection"
filtered_annotations <- v_annotations_df %>%
  filter(sentence_id %in% sentences_with_insurrection$sentence_id)

# Filter for 'nsubj' dependency relations
nsubj_df <- filtered_annotations %>%
  filter(dep_rel == "nsubj")

# Count frequencies of different subjects
subject_freq <- nsubj_df %>%
  group_by(lemma) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# View the most frequent subjects
print(subject_freq)


freq_top10 <- head(subject_freq, 10)


# Plot the frequency of subjects
ggplot(freq_top10, aes(x = reorder(lemma, count), y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Subject") +
  ylab("Frequency") +
  ggtitle("Most Frequent Subjects in Sentences Containing Terms Related to Insurrection")


```

Example of depencency relations with a single sentence: 
```{r}
####################TESTING W/ Single sentence###########################################
sentence <- "By Captain Mason, from South Carolina, we have advice that the inhabitants there have been greatly alarmed with the report of an intended insurrection of the Negroes to massacre the white people throughout that
province, which was to have been put in execution on Christmas Eve, and during the Holidays, but was happily discovered by a Negro woman" #from Virginia Gazette, 1766_03_07

annotations <- udpipe_annotate(ud_model, x = sentence) #english language part of speech tagging model
annotations_df <- as.data.frame(annotations) #stores new POS tagged sentence in dataframe

# View the annotations
print(annotations_df)

# Filter for the term "insurrection" and "Negroes"
keywords <- annotations_df %>%
  filter(str_detect(lemma, "insurrection|Negroes"))

print(keywords)

# Filter for the 'insurrection' and 'Negroes' and their dependencies
relations <- annotations_df %>%
  filter(str_detect(lemma, "insurrection|Negroes"))

print(relations)

# Find and display the dependency relations
relations %>%
  select(token, lemma, dep_rel, head_token_id) %>%
  arrange(head_token_id) %>%
  print()


```

Word Co-Occurence Networks: does not show change over time
```{r}

# Subset to Nouns and Adjectives
target_pos <- c("NOUN", "ADJ")
target_words <- c("insurrection", "insurrections", "plott", "slave", "slaves", "negro", 
                  "negroes", "rebellious", "intended", "massacre", "conspiracy", "conspiracies", "rebel", 
                  "plot", "uprising", "wicked", "ringleader", "king", "leader", "head", "whites") #based on consensus

# Filter for POS tags and nouns/adjectives that follow one another
Cooccurrence <- cooccurrence(v_annotations_df$lemma, relevant = v_annotations_df$upos %in% c("NOUN", "ADJ"), skipgram = 1)


# Create co-occurrence pairs within a 5-word window
window_size <- 5

wordnetwork <- head(Cooccurrence, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )


ggsave("CorpusTwo_NounAdj_coocurrence.svg", plot = last_plot(), width = 6, height = 4)


```