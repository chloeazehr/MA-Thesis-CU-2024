---
title: "PassivePy_Virginia_Gazette_1735_1775"
author: "Chloe Zehr"
date: "2024-09-11"
output: html_document
---


CORPUS TWO DATA OVERVIEW

Collection: Manually collected via keyword searches - based on extensive close readings and secondary research - from History Commons digital archive (https://history-commons.net/). Between 1735, the year of the first Virginia Gazette's founding, and 1775 there are at least 55 articles published in the Virginia Gazette(s) that reference collective enslaved/black unrest. 

Metadata & variables: 
1. newspaper name
2. newspaper_ID_histCommons - only relevant for the Virginia Gazette(s) since there are multiple papers named the same thing, the number references the paper
3. printer - name of the printer
4. print_location - location of the printer
5. date - in the format YYYY_MM_DD
6. written_date - shows the date in "Month Day, Year"
7. year
8. reference_type - this indicates the kind of reference to enslaved resistance in the article can can be one of the following: legislation, declaration, intended insurrection, occurred insurrections, rebels, shipboard resistance, stirring up
9. reference_location - the location referenced in the article regarding enslaved resistance
10. author_informant_gender - if known, records the gender of the identified author or informant of the report 
11. text - contains the full text data of each article 
12. notes - general research notes

Loading/installing necessary packages:
```{r}

library(extrafont)
library(udpipe)
library(dplyr)
library(tm)
library(stringr)
library(ggplot2)
library(NLP)
library(tokenizers)
library(stringi)
library(readxl)
library(svglite)
library(readxl)
library(stringr)

```


Filtering dataset for Virginia Gazette
```{r}

#file.choose()

virginia_gazette <- read_excel("C:\\Users\\chloe\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\Corpora_final\\2024_09_19_VG_enslaved_corp.xlsx", sheet = "VG_resistance_corp")

```

Loading/cleaning/prepping data for PassivePy (a tool that can detect passive voice, see GitHub repo: https://github.com/mitramir55/PassivePy?tab=readme-ov-file)
```{r}
# Specify the column names
text_column <- "text"  # column name
date_column <- "date"  # column name

# Create a directory to save the text files
dir.create("CorpusTwo_text_files", showWarnings = FALSE)

# Iterate over rows and write each text entry to a separate file
for (i in 1:nrow(virginia_gazette)) { 
  # Construct the file name with the date and row number to ensure uniqueness - due to duplicate dates
  v_file_name <- paste0("CorpusTwo_text_files/text_", virginia_gazette[[date_column]][i], "_row_", i, ".txt") #extracts row number to avoid overwriting dates that occur multiple times
  
  # Extract the text for this row and ensure it's a character type, removing line breaks from History Commons formatting
  vtext <- as.character(virginia_gazette[[text_column]][i])
  vtext <- str_replace_all(vtext, "\\r?\\n", " ")  # Remove line breaks 
  
  # Write the cleaned text to a file
  writeLines(vtext, v_file_name) 
}

# List files in the directory to verify
created_files <- list.files("CorpusTwo_text_files")
print(created_files)
print(paste("Number of files created:", length(created_files))) #shows me that the right number was created (should be 55)

```


Creating a Corpus with "tm": 
```{r}
#Loading and sorting Corpus One so that it retains chronological order
# Step 1: List the files and ensure they are sorted by the numeric part of their filenames
file_paths <- list.files("C:/Users/chloe/OneDrive - UCB-O365/Masters Thesis 2024/Computational Text Mining/PassivePy/Corpus_Two/CorpusTwo_text_files", 
                         pattern = "^text_\\d{4}_\\d{2}_\\d{2}_row_\\d+\\.txt$", #matches text file path pattern, i.e. file naming pattern
                         full.names = TRUE)
print(file_paths)

# Step 2: Sort the files based on the numeric portion of the filename
sorted_file_paths <- file_paths[order(as.numeric(gsub("\\D", "", basename(file_paths))))] # I had to add this step because R was incorrectly reordering the text-files based on character so they were no longer chronological 

# Step 3: Create the corpus using the sorted file paths
Corpus2 <- Corpus(VectorSource(lapply(sorted_file_paths, readLines))) #could be used of other analyses

```


Preparing functions:
```{r}
# Function to extract the date from the filename to retain for diachronic analysis
extract_date_from_filename <- function(file_name) {
  # Regex to extract the date part (YYYY_MM_DD)
  date <- str_extract(file_name, "\\d{4}_\\d{2}_\\d{2}") # may have to adjust depending on file naming pattern
  return(date)
}

# Function for tokenizing that iterates over sentences
tokenize_file_sentences <- function(file_path) {
  # Read the file content
  text <- paste(readLines(file_path), collapse = " ")
  
  # Regex pattern to split sentences
  pattern <- "(?<=[^A-Z].[.?;])(?<!\\b(Mr|Ms|Capt|Col|Sir|Maj|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Oct|Nov|Dec|Viz)\\.)\\s+"
  # regex pattern that identifies the punctuation to that should be used to tokenize as well as the patterns were the code should ignore the punctuation, such as honorifics and date abbreviations.
  
  # Tokenize the text into sentences
  sentences <- stri_split_regex(text, pattern) #uses regex pattern to split the text files into sentences
  
  # Return the tokenized sentences as a vector
  return(unlist(sentences))
}

# Custom function to clean sentences by removing integers/numbers and underscores - for these particular sources this was a necessary step to take
clean_sentences <- function(sentence) {
  # Remove all integers and underscores
  cleaned_sentence <- gsub("[0-9]", "", sentence)   # Removes all numbers
  cleaned_sentence <- gsub("_", "", cleaned_sentence) # Removes all underscores
  cleaned_sentence <- gsub("-", "", cleaned_sentence) #Removes hyphens
  
  # Return the cleaned sentence
  return(cleaned_sentence)
}


# List all text files in the directory
file_list <- list.files(path = "C:/Users/chloe/OneDrive - UCB-O365/Masters Thesis 2024/Computational Text Mining/PassivePy/Corpus_Two/CorpusTwo_text_files", pattern = "*.txt", full.names = TRUE) # gets text files from Corpus Two folder (VG resistance corpus)

# Initialize a list to store the tokenized sentences
all_tokenized_sentences <- list()

# Iterate over each file and tokenize the sentences
for (file in file_list) {
  # Tokenize sentences for the current file
  tokenized_sentences <- tokenize_file_sentences(file) # function made above
  
  # Store the result in the list
  all_tokenized_sentences[[file]] <- tokenized_sentences
}

# Creating a dataframe that contains all tokenized sentences and adds a doc_id and date to maintain context/identification
CorpusTwo_sentences_df <- bind_rows(lapply(names(all_tokenized_sentences), function(file_name) {
  # Get tokenized sentences for the current file
  sentences <- all_tokenized_sentences[[file_name]]
  
  # Extract the date from the file name
  file_date <- extract_date_from_filename(file_name)
  
  # Create a dataframe with sentences, the corresponding file name, and extracted date from filename
  data.frame(
    file_name = file_name,
    sentence = sentences,
    date = file_date,  # Add the extracted date
    stringsAsFactors = FALSE
  )
}), .id = "doc_id")

# Further cleaning needed for compatibility with PassivePy/Spacy
# Clean the 'sentence' column to remove integers and underscores
CorpusTwo_sentences_df$sentence <- sapply(CorpusTwo_sentences_df$sentence, clean_sentences)

# Convert the 'sentence' column to character to ensure compatibility with PassivePy streamlit tool
CorpusTwo_sentences_df$sentence <- as.character(CorpusTwo_sentences_df$sentence) # avoiding errors with numbers

# Replace any NA or NULL values in the sentence column with empty strings - this can occur with errors in tokenization
CorpusTwo_sentences_df$sentence[is.na(CorpusTwo_sentences_df$sentence)] <- ""

# Verifying the CSV encoding: 
# Save CSV with UTF-8 encoding (just to extra make sure it works with PassivePy)
write.csv(CorpusTwo_sentences_df, "CorpusTwo_tokenized_sentences_cleaned.csv", row.names = FALSE, fileEncoding = "UTF-8")


```

Documentation for PassivePy results from https://github.com/mitramir55/PassivePy. 
# corpus level (one dataframe)
document : Records in the input data frame
binary : Whether a passive was detected in that document (0 or 1, 1 meaning passive voice was detected)
passive_match(es) : Parts of the document detected as passive
raw_passive_count : Number of passive voices detected in the sentence
raw_passive_sents_count : Number of sentences with passive voice
raw_sentence_count : Number of sentences detected in the document
passive_sents_percentage : Proportion of passive sentences to total number of sentences
date: YYYY_MM_DD, added by me (Zehr, Chloe)

# Sentence level (one dataframe)
docId : Initial index of the record in the input file
sentenceId : The ith sentence in one specific record
sentence : The detected sentence
binary : Whether a passive was detected in that sentence (0 or 1, 1 meaning passive voice was detected)
passive_match(es) : The part of the record detected as passive voice
raw_passive_count : Number of passive forms detected in the sentence
date: YYYY_MM_DD, added by me (Zehr, Chloe)


Analyzing/Visualizing passivePy results from streamlit PassivePy tool (https://passivepy.streamlit.app/)
```{r}

#loading the dataset: 
#file.choose()
CorpusTwo_sentenceLevel_PassPy <- read.csv("C:\\Users\\chloe\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\PassivePy\\Corpus_Two\\CorpusTwo_PassivePy_Sentence_Level.csv") 

#Percentage of sentences with Passive Voice in the VA Gazette articles regarding enslaved resistance (50 articles) from 1735-1775
# the 'binary' column contains 1 for passive, 0 for active sentences
# Calculate the percentage of passive sentences
passive_percentage <- sum(CorpusTwo_sentenceLevel_PassPy$binary == 1) / nrow(VA_sentenceLevel_PassPy) * 100

# Calculate the relative frequency of active v. passive voice
relative_freq <- CorpusTwo_sentenceLevel_PassPy %>%
  group_by(binary) %>%
  summarise(count = n()) %>%
  mutate(relative_frequency = count / sum(count)*100)  # Calculate relative frequency

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
fonts() #verify the font wanted is present

#VISUALIZATION
# for changing font: 
#font_import() #getting fonts from "extrafont" package - look at your console for prompt
loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

# Create geom_col()
ggplot(relative_freq, aes(x = factor(binary), y = relative_frequency, fill = factor(binary))) +
  geom_col() +
  scale_fill_manual(values = c("0" = "gray", "1" = "black"), labels = c("Active", "Passive")) +
  labs(x = "Sentence Type", y = "Percent", fill = "Type") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 14),
    axis.title = element_text(size = 14),        # Axis titles font size
    axis.text = element_text(size = 14),          # Axis text font size
    legend.text = element_text(size = 14),        # Legend text font size
    legend.title = element_text(size = 14)        # Legend title font size
  )

#exporting graph as SVG: 
# Export ggplot as an SVG, SVG is the best for adding into a paper so that you can resize the image without losing quality
ggsave("CorpusTwo_relativeFrq_passive_plot.svg", plot = last_plot(), width = 6, height = 4)


```

Isolating passive voice constructions by context
```{r}

CorpusTwo_passives <- CorpusTwo_sentenceLevel_PassPy %>% 
  filter(all_passives_count >= 1)

sum(CorpusTwo_passives$all_passives_count) #453 passives 

###########contexts
# Extract the relevant columns
CorpusTwo_passives_year <- as.numeric(substr(CorpusTwo_passives$date, 1, 4))  # Extract year if needed
passive_df <- CorpusTwo_passives %>% filter(all_passives_count > 0)

# Define the flexible keywords for the consequences enslaved rebels faced or how white colonists perceived of the outcomes
flexible_keywords <- c("executed", "punished", "hanged", "whipped", "burned", "branded", 
                       "severely dealt", "imprisoned", "sentenced", "condemned", "death", 
                       "trial", "tried", "punishment", "penalty", "lashes", "discovered", 
                       "apprehended", "suppressed", "were brought", "were cut", "were burnt", 
                       "were hung", "was taken", "be tried", "destroyed", "been hung", "was hung", 
                       "half burnt", "was thrown", "was drove", "are exposed", "have been threatened", 
                       "were discovered", "were found", "are killed", "were left", "were kept", 
                       "were immediately told", "be punished", "were disposed", "been misled", 
                       "are detained", "has been laid", "was discovered", "were entirely suppressed", 
                       "having been tried", "is supposed", "was not passed", "found guilty", 
                       "being taken", "was thus saved", "has been discovered", "was found", 
                       "was luckily discovered", "being first stranged", "are fixed", "had been cut", 
                       "by an insurrection of the slaves", "subdued", "occasioned by an insurrection of the slaves", 
                       "have been obliged", "surprised by an insurrection of the negroes", "were killed", 
                       "was lost", "were expended", "were alarmed", "are suspected", "is hoped", 
                       "be subdued", "are now supposed", "has been lately discovered", "have been detected", 
                       "be divided", "was found", "have been committed", "were committed", "being deposed", 
                       "was found", "was become")

# Create a regex pattern from the keywords
pattern <- paste(flexible_keywords, collapse = "|")

# Filter sentences with consequences using flexible keywords
consequences_df <- passive_df %>% 
  filter(str_detect(sentences, regex(pattern, ignore_case = TRUE)))

# Calculate counts and percentages
total_passive_count <- sum(passive_df$all_passives_count, na.rm = TRUE)
consequences_passive_count <- sum(consequences_df$all_passives_count, na.rm = TRUE)
other_passive_count <- total_passive_count - consequences_passive_count

percentage_consequences <- (consequences_passive_count / total_passive_count) * 100
percentage_other <- 100 - percentage_consequences

# Display the results
summary_CorpusTwo <- list(
  Total_Passive_Count = total_passive_count,
  Consequence_Passive_Count = consequences_passive_count,
  Other_Passive_Count = other_passive_count,
  Percentage_Consequence = percentage_consequences,
  Percentage_Other = percentage_other
)

print(summary)

# Analyze consequence-related passive constructions over time
consequences_over_time <- consequences_df %>%
  group_by(year) %>%
  summarise(Consequence_Passive_Count = sum(all_passives_count, na.rm = TRUE))

total_passives_over_time <- passive_df %>%
  group_by(year) %>%
  summarise(Total_Passive_Count = sum(all_passives_count, na.rm = TRUE))

# Merge and calculate percentages
CorpusTwo_passivesCon_overtime <- merge(consequences_over_time, total_passives_over_time, by = "year", all = TRUE) %>%
  mutate(Percentage_Consequence_Passives = (Consequence_Passive_Count / Total_Passive_Count) * 100)


```

Analyzing passives over time: using the "date" column
```{r}


#make a new column that just stores the year: 
CorpusTwo_sentenceLevel_PassPy <- CorpusTwo_sentenceLevel_PassPy %>%
  mutate(year = substr(date, 1, 4))


#new df of passive percentages by year
virginia_yearly_passives <- CorpusTwo_sentenceLevel_PassPy %>% 
  group_by(year) %>%
  summarise(total_sentences = n(),
            passive_sentences = sum(binary),
            passive_percentage = (passive_sentences / total_sentences) * 100)

#plotting the trend overtime: 
ggplot(virginia_yearly_passives, aes(x = year, y = passive_percentage)) +
  geom_col() +
  labs(x = "Year",y = "Percentage of Passive Sentences") +
  theme_minimal() + 
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 14),
    axis.title = element_text(size = 14),        # Axis titles font size
    axis.text = element_text(size = 14),          # Axis text font size
    legend.text = element_text(size = 14),        # Legend text font size
    legend.title = element_text(size = 14)        # Legend title font size
  )

ggsave("CorpusTwo_sentenceLevel_PassivesOverTime.svg", plot = last_plot(), width = 6, height = 4)

```


Documents with the most passives: 
```{r}

# Summarise passive sentences by document
virginia_doc_passives <- VA_sentenceLevel_PassPy_update %>%
  group_by(doc_id) %>%
  summarise(total_passive = sum(binary)) %>%
  arrange(desc(total_passive))

# Plot the result - not a very useful plot besides that it indicates that doc #36 has the most passive sentences
ggplot(virginia_doc_passives, aes(x = reorder(doc_id, -total_passive), y = total_passive)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Document ID", y = "Number of Passive Sentences") +
  theme_minimal() + 
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11), 
    axis.text.y = element_text(margin = margin(r = 20)),  # Add right margin for spacing y-axis labels
    axis.ticks.length = unit(2, "cm")           # Extend the tick marks for better spacing
  )# Legend title font size

ggsave("plot4.svg", plot = last_plot(), width = 6, height = 4)


```
