---
title: "PassivePy_SouthCarolina_Gazette_1732_1775"
author: "Chloe Zehr"
date: "2024-09-20"
output: html_document
---

CORPUS ONE DATA OVERVIEW

Collection: Manually collected via keyword searches - based on extensive close readings and secondary research - from History Commons digital archive (https://history-commons.net/). Between 1732, the year of the first Virginia Gazette's founding, and 1775 there are at least 43 articles published in the South Carolina Gazette that reference collective enslaved/black unrest. 

Metadata & variables: 
1. newspaper name
2. newspaper_ID_histCommons - only relevant for the Virginia Gazette(s) since there are multiple papers named the same thing, the number references the paper
3. printer - name of the printer
4. print_location - location of the printer
5. date - in the format YYYY_MM_DD
6. written_date - shows the date in "Month Day, Year"
7. year
8. reference_type - this indicates the kind of reference to enslaved resistance in the article can can be one of the following: legislation, declaration, intended insurrection, occurred insurrections, rebels, shipboard resistance, stirring up
9. reference_location - the location referenced in the article regarding enslaved resistance
10. author_informant_gender - if known, records the gender of the identified author or informant of the report 
11. text - contains the full text data of each article 
12. notes - general research notes


Loading/installing necessary packages:
```{r}

library(extrafont)
library(udpipe)
library(dplyr)
library(tm)
library(stringr)
library(ggplot2)
library(NLP)
library(tokenizers)
library(stringi)
library(readxl)
library(svglite)
library(readxl)
library(stringr)
library(lubridate)

```


loading Corpus 1: South Carolina Gazette references to enslaved resistance
```{r}

#get full file path
#file.choose()

sc_dat <- read_excel("C:\\Users\\chloe\\OneDrive\\Desktop\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\Corpora_final\\2024_09_19_SCG_enslaved_corp.xlsx", sheet = "SCG_resistance_corp")

sc_dat_select <- sc_dat %>%
  select(newspaper, date, text) # just a little bit of cleaning to make it easier to see desired data

```


Generate text files: 
```{r}

# Specify the column names
text_column <- "text"  # column name
date_column <- "date"  # column name

# Create a directory to save the text files
dir.create("CorpusOne_text_files", showWarnings = FALSE)

# Iterate over rows and write each text entry to a separate file
for (i in 1:nrow(sc_dat_select)) { #using the select data
  # Construct the file name with the date and row number to ensure uniqueness - due to duplicate dates
  sc_file_name <- paste0("CorpusOne_text_files/text_", sc_dat_select[[date_column]][i], "_row_", i, ".txt")
  
  # Extract the text for this row and ensure it's a character type, removing line breaks from History Commons formatting
  sctext <- as.character(sc_dat_select[[text_column]][i])
  sctext <- str_replace_all(sctext, "\\r?\\n", " ")  # Remove line breaks
  
  # Write the cleaned text to a file
  writeLines(sctext, sc_file_name)
}

# List files in the directory to verify
created_files <- list.files("CorpusOne_text_files")
print(created_files)
print(paste("Number of files created:", length(created_files))) #shows me that the right number was created


```


Creating a Corpus with "tm" (and sorting text files to retain chronological order):
```{r}

#Loading and sorting Corpus One so that it retains chronological order
# Step 1: List the files and ensure they are sorted by the numeric part of their filenames
file_paths <- list.files("C:/Users/chloe/OneDrive/Desktop/OneDrive - UCB-O365/Masters Thesis 2024/Computational Text Mining/PassivePy/CorpusOne_text_files", 
                         pattern = "^text_\\d{4}_\\d{2}_\\d{2}_row_\\d+\\.txt$", #matches text file path pattern
                         full.names = TRUE)
print(file_paths)

# Step 2: Sort the files based on the numeric portion of the filename
sorted_file_paths <- file_paths[order(as.numeric(gsub("\\D", "", basename(file_paths))))] # I had to add this step because R was incorrectly reordering the text-files based on character so they were no longer chronological 

# Step 3: Create the corpus using the sorted file paths
Corpus1 <- Corpus(VectorSource(lapply(sorted_file_paths, readLines))) #could be used of other analyses

```


Tokenizing the corpus into sentences for preprocessing: (maintaining Year Data)
```{r}
# Function to extract the date from the filename to retain for diachronic analysis
extract_date_from_filename <- function(file_name) {
  # Regex to extract the date part (YYYY_MM_DD)
  date <- str_extract(file_name, "\\d{4}_\\d{2}_\\d{2}") # may have to adjust depending on file naming pattern
  return(date)
}

# Function for tokenizing that iterates over sentences
tokenize_file_sentences <- function(file_path) {
  # Read the file content
  text <- paste(readLines(file_path), collapse = " ")
  
  # Regex pattern to split sentences
  pattern <- "(?<=[^A-Z].[.?;])(?<!\\b(Mr|Ms|Capt|Col|Sir|Maj|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Oct|Nov|Dec|Viz)\\.)\\s+"
  # regex pattern that identifies the punctuation to that should be used to tokenize as well as the patterns were the code should ignore the punctuation, such as honorifics and date abbreviations.
  
  # Tokenize the text into sentences
  sentences <- stri_split_regex(text, pattern) #uses regex pattern to split the text files into sentences
  
  # Return the tokenized sentences as a vector
  return(unlist(sentences))
}

# Custom function to clean sentences by removing integers/numbers and underscores - for these particular sources this was a necessary step to take
clean_sentences <- function(sentence) {
  # Remove all integers and underscores
  cleaned_sentence <- gsub("[0-9]", "", sentence)   # Removes all numbers
  cleaned_sentence <- gsub("_", "", cleaned_sentence) # Removes all underscores
  cleaned_sentence <- gsub("-", "", cleaned_sentence)
  
  # Return the cleaned sentence
  return(cleaned_sentence)
}

# List all text files in the directory
file_list <- list.files(path = "CorpusOne_text_files", pattern = "*.txt", full.names = TRUE) # gets txt files from Corpus One folder (SCG resistance corpus)

# Initialize a list to store the tokenized sentences
all_tokenized_sentences <- list()

# Iterate over each file and tokenize the sentences
for (file in file_list) {
  # Tokenize sentences for the current file
  tokenized_sentences <- tokenize_file_sentences(file) # function made above
  
  # Store the result in the list
  all_tokenized_sentences[[file]] <- tokenized_sentences
}

# Creating a dataframe that contains all tokenized sentences and adds a doc_id and date to maintain context/identification
CorpusOne_sentences_df <- bind_rows(lapply(names(all_tokenized_sentences), function(file_name) {
  # Get tokenized sentences for the current file
  sentences <- all_tokenized_sentences[[file_name]]
  
  # Extract the date from the file name
  file_date <- extract_date_from_filename(file_name)
  
  # Create a dataframe with sentences, the corresponding file name, and extracted date from filename
  data.frame(
    file_name = file_name,
    sentence = sentences,
    date = file_date,  # Add the extracted date
    stringsAsFactors = FALSE
  )
}), .id = "doc_id")

# Further cleaning needed for compatibility with PassivePy/Spacy
# Clean the 'sentence' column to remove integers and underscores
CorpusOne_sentences_df$sentence <- sapply(CorpusOne_sentences_df$sentence, clean_sentences)

# Convert the 'sentence' column to character to ensure compatibility with PassivePy streamlit tool
CorpusOne_sentences_df$sentence <- as.character(CorpusOne_sentences_df$sentence) # avoiding errors with numbers

# Replace any NA or NULL values in the sentence column with empty strings - this can occur with errors in tokenization
CorpusOne_sentences_df$sentence[is.na(CorpusOne_sentences_df$sentence)] <- ""

# Verifying the CSV encoding: 
# Save CSV with UTF-8 encoding (just to extra make sure it works with PassivePy)
write.csv(CorpusOne_sentences_df, "CorpusOne_tokenized_sentences_cleaned.csv", row.names = FALSE, fileEncoding = "UTF-8")

```


Documentation for PassivePy results from https://github.com/mitramir55/PassivePy. 
# corpus level (one dataframe)
document : Records in the input data frame
binary : Whether a passive was detected in that document (0 or 1, 1 meaning passive voice was detected)
passive_match(es) : Parts of the document detected as passive
raw_passive_count : Number of passive voices detected in the sentence
raw_passive_sents_count : Number of sentences with passive voice
raw_sentence_count : Number of sentences detected in the document
passive_sents_percentage : Proportion of passive sentences to total number of sentences
date: YYYY_MM_DD, added by me (Zehr, Chloe)

# Sentence level (one dataframe)
docId : Initial index of the record in the input file
sentenceId : The ith sentence in one specific record
sentence : The detected sentence
binary : Whether a passive was detected in that sentence (0 or 1, 1 meaning passive voice was detected)
passive_match(es) : The part of the record detected as passive voice
raw_passive_count : Number of passive forms detected in the sentence
date: YYYY_MM_DD, added by me (Zehr, Chloe)


Analyzing/Visualizing passivePy results from streamlit PassivePy tool (https://passivepy.streamlit.app/). 
See GitHub repo: https://github.com/mitramir55/PassivePy.
```{r}

#loading data: 
#file.choose()
CorpusOne_sentenceLevel_PassPy <- read.csv("C:\\Users\\chloe\\OneDrive\\Desktop\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\PassivePy\\Corpus_One\\CorpusOne_PassivePy_SentenceLevel.csv")

#Percentage of sentences with Passive Voice in the SCG Gazette articles regarding enslaved resistance (43 articles) from 1732-1775
# the 'binary' column contains 1 for passive, 0 for active sentences
# Calculate the percentage of passive sentences
passive_percentage <- sum(CorpusOne_sentenceLevel_PassPy$binary == 1) / nrow(CorpusOne_sentenceLevel_PassPy) * 100 #calculating percentage of passive sentences
#34.38% of the sentences contain a passive construction, but this only gives you a single percentage


# Calculating the relative frequency of passive vs. active voice in Corpus One
relative_freq <- CorpusOne_sentenceLevel_PassPy %>%
  group_by(binary) %>%
  summarise(count = n()) %>%
  mutate(relative_frequency = count / sum(count)*100)  # Calculate relative frequency



#VISUALIZATION
# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

# Create geom_col()
ggplot(relative_freq, aes(x = factor(binary), y = relative_frequency, fill = factor(binary))) +
  geom_col() +
  scale_fill_manual(values = c("0" = "gray", "1" = "black"), labels = c("Active", "Passive")) +
  labs(x = "Sentence Type", y = "Percent", fill = "Type") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )

#exporting graph as SVG: 
# Export ggplot as an SVG, SVG is the best for adding into a paper so that you can resize the image without losing quality
ggsave("CorpusOne_relativeFrq_passive_plot.svg", plot = last_plot(), width = 6, height = 4)


```



Isolating passive voice constructions by context
```{r}
# Load necessary libraries
library(dplyr)
library(stringr)

# Extract the year from the date column formatted as YYYY_MM_DD
CorpusOne_passivesyear <- as.numeric(str_extract(df_new$date, "\\d{4}"))

# Filter sentences with passive phrases
passive_df <- CorpusOne_sentenceLevel_PassPy %>% filter(all_passives_count > 0)

# Define the flexible keywords for filtering consequences-related passive phrases
flexible_keywords <- c("executed", "punished", "hanged", "whipped", "burned", "branded", 
                       "severely dealt", "imprisoned", "sentenced", "condemned", "death", 
                       "trial", "tried", "punishment", "penalty", "lashes", "discovered", 
                       "apprehended", "suppressed", "were brought", "were cut", "were burnt", 
                       "were hung", "was taken", "be tried", "destroyed", "been hung", "was hung", 
                       "half burnt", "was thrown", "was drove", "are exposed", "have been threatened", 
                       "were discovered", "were found", "are killed", "were left", "were kept", 
                       "were immediately told", "be punished", "were disposed", "been misled", 
                       "are detained", "has been laid", "was discovered", "were entirely suppressed", 
                       "having been tried", "is supposed", "was not passed", "found guilty", 
                       "being taken", "was thus saved", "has been discovered", "was found", 
                       "was luckily discovered", "being first stranged", "are fixed", "had been cut", 
                       "by an insurrection of the slaves", "subdued", "occasioned by an insurrection of the slaves", 
                       "have been obliged", "surprised by an insurrection of the negroes", "were killed", 
                       "was lost", "were expended", "were alarmed", "are suspected", "is hoped", 
                       "be subdued", "are now supposed", "has been lately discovered", "have been detected", 
                       "be divided", "was found", "have been committed", "were committed", "being deposed", 
                       "was found", "was become")

# Create a regex pattern from the keywords
pattern <- paste(flexible_keywords, collapse = "|")

# Filter sentences with consequences using flexible keywords
consequences_df <- passive_df %>% 
  filter(str_detect(sentences, regex(pattern, ignore_case = TRUE)))

# Calculate counts and percentages
total_passive_count <- sum(passive_df_new$all_passives_count, na.rm = TRUE)
consequences_passive_count <- sum(consequences_df_new$all_passives_count, na.rm = TRUE)
other_passive_count <- total_passive_count_new - consequences_passive_count_new

percentage_consequences <- (consequences_passive_count_new / total_passive_count_new) * 100
percentage_other <- 100 - percentage_consequences_new

# Display the summary
summary_CorpusOne <- list(
  Total_Passive_Count = total_passive_count_new,
  Consequence_Passive_Count = consequences_passive_count_new,
  Other_Passive_Count = other_passive_count_new,
  Percentage_Consequence = percentage_consequences_new,
  Percentage_Other = percentage_other_new
)

print(summary_new)

# Analyze consequence-related passive constructions over time
consequences_over_time <- consequences_df %>%
  group_by(year) %>%
  summarise(Consequence_Passive_Count = sum(all_passives_count, na.rm = TRUE))

total_passives_over_time <- passive_df %>%
  group_by(year) %>%
  summarise(Total_Passive_Count = sum(all_passives_count, na.rm = TRUE))

# Merge and calculate percentages
CorpusOne_passivesCon_overtime <- merge(consequences_over_time, total_passives_over_time, by = "year", all = TRUE) %>%
  mutate(Percentage_Consequence_Passives = (Consequence_Passive_Count / Total_Passive_Count) * 100)


```


Analyzing passives over time: using the "date" column
```{r}

#make a new column that just stores the year: 
CorpusOne_sentenceLevel_PassPy <- CorpusOne_sentenceLevel_PassPy %>%
  mutate(year = substr(date, 1, 4))

#new df of passive percentages by year
CorpusOne_yearly_passives <- CorpusOne_sentenceLevel_PassPy %>% 
  group_by(year) %>%
  summarise(total_sentences = n(),
            passive_sentences = sum(binary),
            passive_percentage = (passive_sentences / total_sentences) * 100)

#plotting the trend overtime: 
ggplot(CorpusOne_yearly_passives, aes(x = year, y = passive_percentage)) +
  geom_col() +
  labs(x = "Year",y = "Percentage of Passive Sentences") +
  theme_minimal() + 
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )

ggsave("CorpusOne_yearlyPassives_plot.svg", plot = last_plot(), width = 6, height = 4)

```



EDA on most common passive phrases: 
```{r}
#requires the Corpus One sentence-level PassivePy data
#cleaned phrases: includes date information and each phrase has been separated into its own row
passive_counts_cleaned <- read_excel("C:\\Users\\chloe\\OneDrive\\Desktop\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\PassivePy\\Corpus_One\\CorpusOne_PassivePhrases_counts.xlsx", sheet = "sheet1")

passive_counts_cleaned <- passive_counts_cleaned %>%
  mutate(year = as.numeric(format(ymd(date), "%Y"))) #adding year column

#generating a timeline
top_phrases <- passive_counts_cleaned %>%
  group_by(all_passives) %>%
  summarise(total_count = sum(count)) %>%
  arrange(desc(total_count)) %>%
  slice_head(n = 20) %>%
  pull(all_passives)

top20 <- passive_counts_cleaned %>%
  filter(all_passives %in% top_phrases) #creating new object that just selects the top phrases

#aggregating data by date and phrase: 
top20_ag <- top20 %>%
  group_by(year, all_passives) %>%
  summarise(count = sum(count), .groups = "drop")

#crude graph of phrases used over time
ggplot(top20_ag, aes(x = year, y = count, color = all_passives)) +
  geom_line(size = 1)

#visualizing phrases that occur more than 3 times 
over3 <- passive_counts_cleaned %>%
  filter(count >= 3) %>%
  arrange(count)

ggplot(over3) + 
  geom_col(mapping = aes(x = count, y = all_passives))


```


