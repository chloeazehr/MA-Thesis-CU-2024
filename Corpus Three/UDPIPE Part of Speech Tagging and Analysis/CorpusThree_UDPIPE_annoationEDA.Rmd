---
title: "CorpusThree_UDPIPE_annotation_EDA"
author: "Chloe Zehr"
date: "2024-10-02"
output: html_document
---

CORPUS THREE DATA OVERVIEW

Collection: Manually collected via keyword searches - based on extensive close readings and secondary research - from History Commons digital archive (https://history-commons.net/). This corpus contains articles from between 1732 and 1775 published in the South Carolina Gazette and the Virginia Gazette(s) that reference resistance in the form of insurrections, conspiracies, plots, and rebellions that are not related to enslaved Africans and people of African descent. It contains 320 articles. This corpus is meant for content analysis, thus it is randomly sampled via keyword searches and contains at most 10 articles from every year (I tried to balance it by having 5 articles from the SCG and 5 from the VG for every year, however, there are a lot of years where only one paper has references to insurrections, rebellions, and conspiracies), thus it remains somewhat unbalanced.

Metadata & variables: (NOTE: this corpus differs from Corpus One and Corpus Two due to labor and time constraints)
1. newspaper name
2. newspaper_ID_histCommons - only relevant for the Virginia Gazette(s) since there are multiple papers named the same thing, the number references the paper
3. printer - name of the printer
4. print_location - location of the printer
5. date - in the format YYYY_MM_DD
6. written_date - shows the date in "Month Day, Year" (this corpus contains another column with this info due to corrections)
7. year
8. text - contains the full text data of each article 
9. notes - general research notes
10. resistance_tag - lets me analyze all three corpora together and to mark which ones are related to enslaved Africans and people of African descent


Loading necessary libraries:
```{r}

library(udpipe) #part of speech tagging
library(tidyverse) #general utility and tokenization
library(dplyr) #general utility
library(tm) #text mining
library(stringr) #manipulating strings
library(ggplot2) #basic visualizations
library(NLP) #natural language processing tools
library(ggraph) # for graphing cooccurrence
library(igraph) # for graphing cooccurrence
library(readxl) #for reading excel sheets
library(extrafont) #for fonts
library(gganimate) #animating data - this is something I will try more in the future
library(widyr) #for pairwise
library(ggrepel) #for graphing aesthetics with word
library(quanteda) #works with text data and has a built in concordance feature
library(textstem) #lemmatizing for concordance

```

loading data:
```{r}

#file.choose()
CorpusThree <- read.csv("C:\\Users\\chloe\\OneDrive\\Desktop\\OneDrive - UCB-O365\\Masters Thesis 2024\\Computational Text Mining\\Corpora_final\\Corpus_Three.csv")

```


EDA & Concordance: 
```{r}

#concordance analyses: 
CorpusThree$text <- iconv(CorpusThree$text, from = "latin1", to = "UTF-8", sub = "") #needs UTF-8 encoding
CorpusThree$text_lemma <- lemmatize_strings(CorpusThree$text)
corpus_lemma <- corpus(CorpusThree, text_field = "text_lemma")
tokens_lemma <- quanteda::tokens(corpus_lemma, remove_punct = TRUE)
concordance_dfm <- dfm(tokens_lemma) #not necessary but could lend itself to further research

#exploring target words:
#insurrection
insurrections_con <- kwic(tokens_lemma, pattern = "insurrection", window = 6) 
print(insurrections_con)

#we
we_con <- kwic(tokens_lemma, pattern = "we", window = 6) 
print(we_con)

#our
our_con <- kwic(tokens_lemma, pattern = "our", window = 6) 
print(our_con)

#suppress
suppress_con <- kwic(tokens_lemma, pattern = "suppress", window = 6) 
print(suppress_con)

subdue_con <- kwic(tokens_lemma, pattern = "subdue", window = 6) 
print(suppress_con)

prevent_con <- kwic(tokens_lemma, pattern = "prevent", window = 6) 
print(suppress_con)

```


Word co-occurence over time (POS tagging with UDPIPE)
1. loading data and model:
```{r}
#selecting data
text_time3 <- CorpusThree %>% 
  select(text, year)

#load UDPIPE language model if needed: 
model <- udpipe_download_model(language = "english", overwrite = FALSE)
ud_model <- udpipe_load_model(model$file_model) 
```

2. Annotating the corpus while maintaining year information: (had to add assurance for UTF-8 encoding)
```{r}
# Function to annotate text and add year information
pos_annotate_year <- function(text, year, model) {
  text_utf8 <- iconv(text, from = "latin1", to = "UTF-8", sub = "")
  annotated <- as.data.frame(udpipe_annotate(model, x = text_utf8))
  annotated$year <- year
  return(annotated)
}

# Apply annotation to each text and maintain year information
text_time3$annotation_df <- mapply(pos_annotate_year, text_time3$text, text_time3$year, 
                                   MoreArgs = list(model = ud_model), SIMPLIFY = FALSE)

```

3. Removing any invalid annotations - to avoid errors
```{r}

# Check for invalid annotations
invalid_annotations <- sapply(text_time3$annotation_df, function(x) is.null(x) || nrow(x) == 0)

# Filter the data frame based on valid annotations
valid_indices <- which(!invalid_annotations)
text_time3 <- text_time3[valid_indices, ]

# Print out the texts that caused problems
if (any(invalid_annotations)) {
  print("Texts with invalid annotations:")
  print(text_time$text[invalid_annotations])
}

```

4. Combining all the annotated data for each row (or text) into one df that maintains year information
```{r}
# Use bind_rows to combine the list of dataframes - this can bypass issues with memory management and relatively large dataframes like those of UDPIPE
CorpusThree_annotated_data <- bind_rows(text_time3$annotation_df)

# Inspect the first few rows
head(CorpusThree_annotated_data)

#making all lemmas lowercase
CorpusThree_annotated_data$lemma <- tolower(CorpusThree_annotated_data$lemma) #lemmas lowercase
CorpusThree_annotated_data$token <- tolower(CorpusThree_annotated_data$token) #tokens lowercase
```


5. Visualize in 10-year intervals: (NOTE: there are no articles in the Virginia Gazette(s) that reference collective enslaved resistance until 1737 even though William Parks began publishing in 1735. In addition, there are years and decades with very few articles, sometimes zero or only 1, making noun and adjective co-occurence not a very useful measurement since many terms were hapaxes in the article). 

1732-1742
```{r}
annotated_1732_1742 <- CorpusThree_annotated_data %>% filter(year >= 1732 & year <= 1742)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1732_1742 <- cooccurrence(annotated_1732_1742$lemma, relevant = annotated_1732_1742$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1732_1742, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_NOUN_ADJ_cooc_1732_1734.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1732_1742 <- cooccurrence(annotated_1732_1742$token, relevant = annotated_1732_1742$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1732_1742, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_PRON_VERB_cooc_1732_1742.svg", plot = last_plot(), width = 6, height = 4)

```

1743-1753
```{r}
annotated_1743_1753 <- CorpusThree_annotated_data %>% filter(year >= 1743 & year <= 1753)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1743_1753 <- cooccurrence(annotated_1743_1753$lemma, relevant = annotated_1743_1753$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1743_1753, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_noun_adj_cooc_1743_1753.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1743_1753 <- cooccurrence(annotated_1743_1753$token, relevant = annotated_1743_1753$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1743_1753, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_pron_verb_cooc_1743_1753.svg", plot = last_plot(), width = 6, height = 4)

```

1754-1764
```{r}
annotated_1754_1764 <- CorpusThree_annotated_data %>% filter(year >= 1754 & year <= 1764)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1754_1764 <- cooccurrence(annotated_1754_1764$lemma, relevant = annotated_1754_1764$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1754_1764, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_noun_adj_cooc_1754_1764.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1754_1764 <- cooccurrence(annotated_1754_1764$token, relevant = annotated_1754_1764$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1754_1764, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_pron_verb_cooc_1754_1764.svg", plot = last_plot(), width = 6, height = 4)

```

1765-1775
```{r}

annotated_1765_1775 <- CorpusThree_annotated_data %>% filter(year >= 1765 & year <= 1775)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1765_1775 <- cooccurrence(annotated_1765_1775$lemma, relevant = annotated_1765_1775$upos %in% c("NOUN", "ADJ"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1765_1775, 30) #because all bigrams only occur once I expanded to include insurrection reference; not a very useful visualization
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_noun_adj_cooc_1765_1775.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1765_1775 <- cooccurrence(annotated_1765_1775$token, relevant = annotated_1765_1775$upos %in% c("PRON", "VERB", "ADV"), skipgram = 1) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1765_1775, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusThree_pron_verb_cooc_1765_1775.svg", plot = last_plot(), width = 6, height = 4)

```


The use of pronouns:
```{r}
#general pronouns (pronouns, indefinite pronouns, demonstrative pronouns)
annotated_pron <- CorpusThree_annotated_data %>% filter(upos == "PRON")
annotated_pron <- annotated_pron %>% mutate(token = str_to_lower(token))
annotated_pron_count <- annotated_pron %>% 
  group_by(token) %>%
  count(token) %>%
  arrange(desc(n))

pron_top30 <- head(annotated_pron_count, 30)

#most frequent pronouns in the corpus: 
ggplot(pron_top30) + 
  geom_col(mapping = aes(x = n, y = fct_reorder(token, n))) + 
  labs(x = "Count", y = "Pronoun") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )

ggsave("CorpusThree_gen_pronouns.svg", plot = last_plot(), width = 6, height = 4)


```

The types of articles that contain "we": 
```{r}

#primarily editorials

we_filtered <- CorpusThree_annotated_data %>% 
  filter(str_detect(str_to_lower(text), "\\b(we|us|our)\\b")) #makes lowercase and uses regex to find rows that contain forms of "we" 

we_count <- we_filtered %>% 
  group_by(reference_type) %>%
  count(reference_type) %>% 
  arrange(desc(n))

```


Examining sentences that contain target words: Syntactic Dependency Relationships
```{r}
# dependency relations changed over time 
CorpusThree_target_lemmas <- c("insurrection", "conspiracy", "rebellion", "rebel", "revolt", "rebellious", "people", "inhabitants", "subjects", "general", "dangerous")

CorpusThree_filtered_dat <- CorpusThree_annotated_data %>%
  filter(lemma %in% CorpusThree_target_lemmas)

CorpusThree_dep_trends <- CorpusThree_filtered_dat %>%
  group_by(year, lemma, dep_rel) %>%
  summarise(count = n(), .groups = "drop")

# Plot dependency relationships over time
ggplot(CorpusThree_dep_trends, aes(x = year, y = count, fill = dep_rel)) +
  geom_col() +
  facet_wrap(~ lemma, scales = "fixed") +
  theme_minimal() + 
  labs(x = "Year", y = "Count") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), #adding 45 degree angle to x-axis
    axis.ticks.x = element_line(size = 0.5), #x-axis ticks
    panel.spacing = unit(1, "lines"), #adding more space between each facet
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )


ggsave("CorpusThree_dep_rel_Counts.svg", plot = last_plot(), width = 6, height = 4)


```

