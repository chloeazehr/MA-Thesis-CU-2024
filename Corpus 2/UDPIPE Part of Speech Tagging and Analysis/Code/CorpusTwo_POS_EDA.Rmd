---
title: "UDPIPE_Virginia_Resistance_Corpus"
author: "Chloe Zehr"
date: "2024-09-11"
output: html_document
---

CORPUS TWO DATA OVERVIEW

Collection: Manually collected via keyword searches - based on extensive close readings and secondary research - from History Commons digital archive (https://history-commons.net/). Between 1735, the year of the first Virginia Gazette's founding, and 1775 there are at least 55 articles published in the Virginia Gazette(s) that reference collective enslaved/black unrest. 

Metadata & variables: 
1. newspaper name
2. newspaper_ID_histCommons - only relevant for the Virginia Gazette(s) since there are multiple papers named the same thing, the number references the paper
3. printer - name of the printer
4. print_location - location of the printer
5. date - in the format YYYY_MM_DD
6. written_date - shows the date in "Month Day, Year"
7. year
8. reference_type - this indicates the kind of reference to enslaved resistance in the article can can be one of the following: legislation, declaration, intended insurrection, occurred insurrections, rebels, shipboard resistance, stirring up
9. reference_location - the location referenced in the article regarding enslaved resistance
10. author_informant_gender - if known, records the gender of the identified author or informant of the report 
11. text - contains the full text data of each article 
12. notes - general research notes

Loading necessary libraries:
```{r}

library(udpipe) #part of speech tagging
library(tidyverse) #general utility and tokenization
library(dplyr) #general utility
library(tm) #text mining
library(stringr) #manipulating strings
library(ggplot2) #basic visualizations
library(NLP) #natural language processing tools
library(ggraph) # for graphing cooccurrence
library(igraph) # for graphing cooccurrence
library(readxl) #for reading excel sheets
library(extrafont) #for fonts
library(gganimate) #animating data - this is something I will try more in the future
library(widyr) #for pairwise
library(ggrepel) #for graphing aesthetics with word
library(quanteda) #works with text data and has a built in concordance feature
library(textstem) #lemmatizing for concordance

```

loading data:
```{r}

#file.choose()

#loading dataset into environment
CorpusTwo <- read.csv("C:\\Users\\chloe\\OneDrive\\Desktop\\OneDrive - UCB-O365\\Masters Thesis 2024\\GitHub Repo\\Corpus Two\\Data\\Corpus_Two.csv")

```


General exploratory data analysis (EDA) of reference types in the data
```{r}

#reference type categories in reports that make mention of enslaved resistance
refence_type_count <- CorpusTwo %>% 
  select(reference_type) %>% 
  group_by(reference_type) %>%
  count(reference_type) %>%
  arrange(desc(n))

```


Generating basic concordances for target words
```{r}
#concordance analyses: 

CorpusTwo$text <- iconv(CorpusTwo$text, from = "latin1", to = "UTF-8", sub = "") #needs UTF-8 encoding
CorpusTwo$text_lemma <- lemmatize_strings(CorpusTwo$text)
corpus_lemma <- corpus(CorpusTwo, text_field = "text_lemma")
tokens_lemma <- quanteda::tokens(corpus_lemma, remove_punct = TRUE)
concordance_dfm <- dfm(tokens_lemma)

#exploring target words:
#insurrection
insurrections_con <- kwic(tokens_lemma, pattern = "insurrection", window = 6) #kwic() finds keywords in context in a specified window 

#we
we_con <- kwic(tokens_lemma, pattern = "we", window = 6) 

#our
our_con <- kwic(tokens_lemma, pattern = "our", window = 6) 

#suppress
suppress_con <- kwic(tokens_lemma, pattern = "suppress", window = 6) 

#subdue
subdue_con <- kwic(tokens_lemma, pattern = "subdue", window = 6) 

#prevent
prevent_con <- kwic(tokens_lemma, pattern = "prevent", window = 6)

#black
black_con <- kwic(tokens_lemma, pattern = "black", window = 6) 

#african
african_con <- kwic(tokens_lemma, pattern = "africa", window = 6) 

```


Word co-occurence over time (POS tagging with UDPIPE)
1. loading data and model:
```{r}
#selecting data
text_time2 <- CorpusTwo %>% #subsetting data to text and year columns
  select(text, year)

#load UDPIPE language model if needed: 
model <- udpipe_download_model(language = "english", overwrite = FALSE)
ud_model <- udpipe_load_model(model$file_model) 
```


2. Annotating the corpus while maintaining year information: (had to add assurance for UTF-8 encoding)
```{r}
# Creating new function for UDPIPE annotation that maintains year information
pos_annotate_year <- function(text, year, model) {
  text_utf8 <- iconv(text, from = "latin1", to = "UTF-8", sub = "")
  annotated <- as.data.frame(udpipe_annotate(model, x = text_utf8))
  annotated$year <- year
  return(annotated)
}

# Applies annotation to each text while maintaining year information for each annotated unit
text_time2$annotation_df <- mapply(pos_annotate_year, text_time2$text, text_time2$year, 
                                   MoreArgs = list(model = ud_model), SIMPLIFY = FALSE)

```


3. Removing any invalid annotations - to avoid errors
```{r}

# Checking for invalid annotations
invalid_annotations <- sapply(text_time2$annotation_df, function(x) is.null(x) || nrow(x) == 0)

#Filtering out any invalid annotations
valid_indices <- which(!invalid_annotations)
text_time2 <- text_time[valid_indices, ]

# Print out the texts that caused problems/had invalid annotations
if (any(invalid_annotations)) {
  print("Texts with invalid annotations:")
  print(text_time$text[invalid_annotations])
}

```


4. Combining all the annotated data for each row (or text) into one dataframe that maintains year information
```{r}

# Use bind_rows to combine the list of dataframes - this can bypass issues with memory management and relatively large dataframes like those of UDPIPE
CorpusTwo_annotated_data <- bind_rows(text_time2$annotation_df)

# view the first few rows of the new annotated dataset
head(CorpusTwo_annotated_data)

#making all lemmas lowercase to account for duplications (R is generally case sensitive)
CorpusTwo_annotated_data$lemma <- tolower(CorpusTwo_annotated_data$lemma) #lemmas lowercase
CorpusTwo_annotated_data$token <- tolower(CorpusTwo_annotated_data$token) #tokens lowercase

#saving data as csv
write.csv(CorpusTwo_annotated_data, "C:/Users/chloe/OneDrive/Desktop/OneDrive - UCB-O365/Masters Thesis 2024/GitHub Repo/Corpus Two/UDPIPE Part of Speech Tagging and Analysis/Data/CorpusTwo_Annotated_Data.csv")

```


5. Visualize in 10-year intervals: (NOTE: there are no articles in the Virginia Gazette(s) that reference collective enslaved resistance until 1736. In addition, there are years and decades with very few articles, sometimes zero or only 1, making noun and adjective co-occurence not a very useful measurement since many terms were hapaxes, words that only occur once, in the article). 

1735-1745
```{r}
annotated_1735_1745 <- CorpusTwo_annotated_data %>% filter(year >= 1735 & year <= 1745) #subsetting ten year period

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1735_1745 <- cooccurrence(annotated_1735_1745$lemma, relevant = annotated_1735_1745$upos %in% c("NOUN", "ADJ"), skipgram = 20) #skipgram model; shallow neural network algorithm, set to find co-occurences within a 20 word window

# Create co-occurrence pairs within a 5-word window (not necessary)
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1735_1745, 75) #looks at 75 most commonly co-occurring, this changes based on legibility needs for the visualization
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_NOUN_ADJ_cooc_1735_1745.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1735_1745 <- cooccurrence(annotated_1735_1745$token, relevant = annotated_1735_1745$upos %in% c("PRON", "VERB"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1735_1745, 100)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_PRON_VERB_cooc_1735_1745.svg", plot = last_plot(), width = 6, height = 4)



#PRONOUNS, Nouns (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_noun_cooc_1735_1745 <- cooccurrence(annotated_1735_1745$token, relevant = annotated_1735_1745$upos %in% c("PRON", "NOUN"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_noun_cooc_1735_1745, 75)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

ggsave("CorpusTwo_PRON_NOUN_cooc_1735_1745.svg", plot = last_plot(), width = 7, height = 5)

```


1746-1756
```{r}
annotated_1746_1756 <- CorpusTwo_annotated_data %>% filter(year >= 1746 & year <= 1756)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1746_1756 <- cooccurrence(annotated_1746_1756$lemma, relevant = annotated_1746_1756$upos %in% c("NOUN", "ADJ"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1746_1756, 100)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_noun_adj_cooc_1746_1756.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1746_1756 <- cooccurrence(annotated_1746_1756$token, relevant = annotated_1746_1756$upos %in% c("PRON", "VERB", "ADV"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1746_1756, 30) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_pron_verb_cooc_1746_1756.svg", plot = last_plot(), width = 6, height = 4)

```


1757-1767
```{r}
annotated_1757_1767 <- CorpusTwo_annotated_data %>% filter(year >= 1757 & year <= 1767)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1757_1767 <- cooccurrence(annotated_1757_1767$lemma, relevant = annotated_1757_1767$upos %in% c("NOUN", "ADJ"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1757_1767, 50)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_noun_adj_cooc_1757_1767.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS, ADVERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1757_1767 <- cooccurrence(annotated_1757_1767$token, relevant = annotated_1757_1767$upos %in% c("PRON", "VERB"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1757_1767, 100) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_pron_verb_cooc_1757_1767.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, Nouns (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_noun_cooc_1757_1767 <- cooccurrence(annotated_1757_1767$token, relevant = annotated_1757_1767$upos %in% c("PRON", "NOUN"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_noun_cooc_1757_1767, 75)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

ggsave("CorpusTwo_PRON_NOUN_cooc_1757_1767.svg", plot = last_plot(), width = 7, height = 5)

```

1768-1775
```{r}

annotated_1768_1775 <- CorpusTwo_annotated_data %>% filter(year >= 1768 & year <= 1775)

#NOUNS AND ADJECTIVES
# Filter for POS tags and nouns/adjectives that follow one another
noun_adj_cooc_1768_1775 <- cooccurrence(annotated_1768_1775$lemma, relevant = annotated_1768_1775$upos %in% c("NOUN", "ADJ"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(noun_adj_cooc_1768_1775, 50) #because all bigrams only occur once I expanded to include insurrection reference; not a very useful visualization
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_noun_adj_cooc_1768_1775.svg", plot = last_plot(), width = 6, height = 4)


#PRONOUNS, VERBS (switch to token in order to account for lemmas of pronouns like "we" and "our")
pron_verb_cooc_1768_1775 <- cooccurrence(annotated_1768_1775$token, relevant = annotated_1768_1775$upos %in% c("PRON", "VERB", "ADV"), skipgram = 20) #skipgram model; shallow neural network algorithm

# Create co-occurrence pairs within a 5-word window
#window_size <- 5

# for changing font: 
#font_import() #getting fonts from "extrafont" package
#loadfonts(device = "win")  # needed for Windows
#fonts() #verify the font wanted is present

wordnetwork <- head(pron_verb_cooc_1768_1775, 75) #isolates 30 most frequent co-occurrences
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "#ed9de9") +
  geom_node_text(aes(label = name), col = "black", size = 4, family = "Times New Roman") +
  theme_void() +  # This will remove the axes and grid
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    legend.text = element_text(size = 11),       # Legend text font size
    legend.title = element_text(size = 11)       # Legend title font size
  )

#saves the most recent graph as a .svg which can be resized without using quality in Word
ggsave("CorpusTwo_pron_verb_cooc_1768_1775.svg", plot = last_plot(), width = 6, height = 4)

```


The use of pronouns:
```{r}
#general pronouns (pronouns, indefinite pronouns, demonstrative pronouns)
annotated_pron <- CorpusTwo_annotated_data %>% filter(upos == "PRON")
annotated_pron <- annotated_pron %>% mutate(token = str_to_lower(token))
annotated_pron_count <- annotated_pron %>% 
  group_by(token) %>%
  count(token) %>%
  arrange(desc(n))

pron_top30 <- head(annotated_pron_count, 30)

#most frequent pronouns in the corpus: 
ggplot(pron_top30) + 
  geom_col(mapping = aes(x = n, y = fct_reorder(token, n))) + 
  labs(x = "Count", y = "Pronoun") + 
  theme_minimal() +
  theme(text = element_text(family = "Times New Roman")) +
  theme(
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )

ggsave("CorpusTwo_gen_pronouns.svg", plot = last_plot(), width = 6, height = 4)

#FOR PRONOUN USAGE OVER TIME SEE RMD FOR CORPUS ONE


```

The types of articles that contain "we": 
```{r}

#primarily editorials

we_filtered <- Corpus_Two %>% 
  filter(str_detect(str_to_lower(text), "\\b(we|us|our)\\b")) #makes lowercase and uses regex to find rows that contain forms of "we" 

we_count <- we_filtered %>% #counts reference types and sorts by frequency of "we"
  group_by(reference_type) %>%
  count(reference_type) %>% 
  arrange(desc(n))

```


Examining sentences that contain target words: Syntactic Dependency Relationships
```{r}
# dependency relations changed over time between the lemmas insurrection, rebel, rebellious, negro, slave
CorpusTwo_target_lemmas <- c("insurrection", "slave", "rebel", "rebellious", "negro", "negroes") #note that slave is the lemma for "slaves" 

CorpusTwo_filtered_dat <- CorpusTwo_annotated_data %>%
  filter(lemma %in% CorpusTwo_target_lemmas)

CorpusTwo_dep_trends <- CorpusTwo_filtered_dat %>%
  group_by(year, lemma, dep_rel) %>%
  summarise(count = n(), .groups = "drop")

CorpusTwo_dep_freq <- CorpusTwo_dep_trends %>%
  mutate(perc = count/sum(count)*100)

CorpusTwo_dep_freq_select <- CorpusTwo_dep_freq %>% filter(year <= 1749)
CorpusTwo_dep_freq_select <- CorpusTwo_dep_freq_select %>% filter(dep_rel != "flat" & dep_rel != "root")

# Plot dependency relationships over time
ggplot(CorpusTwo_dep_freq_select, aes(x = year, y = perc, fill = dep_rel)) +
  geom_col() +
  facet_wrap(~ lemma, scales = "fixed") +
  theme_minimal() + 
  labs(x = "Year", y = "% frequency") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), #adding 45 degree angle to x-axis
    axis.ticks.x = element_line(size = 0.5), #x-axis ticks
    panel.spacing = unit(1, "lines"), #adding more space between each facet
    text = element_text(family = "Times New Roman", size = 11),
    axis.title = element_text(size = 11),        # Axis titles font size
    axis.text = element_text(size = 11),          # Axis text font size
    legend.text = element_text(size = 11),        # Legend text font size
    legend.title = element_text(size = 11)        # Legend title font size
  )


#ggsave("CorpusTwo_dep_rel_Counts.svg", plot = last_plot(), width = 6, height = 4)


```
